# 网格搜索

超参数调优中的网格搜索（Grid Search）是一种系统化的参数优化方法，通过遍历所有预设的超参数组合来寻找最优模型配置。以下是其核心原理、步骤、优缺点及实际应用的详细介绍：



## **1. 核心原理**
网格搜索通过**穷举所有可能的超参数组合**，逐一训练模型并评估性能，最终选择在验证集上表现最佳的参数组合。  
- **参数空间定义**：预先为每个超参数设定一组候选值（如学习率 `[0.01, 0.1, 1]`）。  
- **笛卡尔积生成组合**：将所有参数的候选值进行全排列，生成所有可能的参数组合。  
- **交叉验证评估**：对每个组合使用交叉验证（如k折）计算平均性能，避免过拟合。  



## **2. 具体步骤**
### **步骤1：定义超参数空间**
```python
param_grid = {
    'n_estimators': [50, 100, 200],        # 决策树数量
    'max_depth': [3, 5, 7],               # 树的最大深度
    'learning_rate': [0.01, 0.1, 0.2]     # 学习率
}
```

### **步骤2：选择模型与评估指标**
- 模型：如随机森林、XGBoost等。  
- 评估指标：如分类任务的准确率（`accuracy`）或回归任务的均方误差（`MSE`）。

### **步骤3：执行网格搜索**
使用交叉验证（如Scikit-learn的 `GridSearchCV`）：
```python
from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import RandomForestClassifier

model = RandomForestClassifier()
grid_search = GridSearchCV(
    estimator=model,
    param_grid=param_grid,
    scoring='accuracy',
    cv=5,            # 5折交叉验证
    n_jobs=-1        # 并行计算（使用所有CPU核心）
)
grid_search.fit(X_train, y_train)
```

### **步骤4：获取最优参数与模型**
```python
print("最佳参数组合:", grid_search.best_params_)
print("最佳模型得分:", grid_search.best_score_)
best_model = grid_search.best_estimator_
```



## **3. 优点与缺点**
| **优点**                          | **缺点**                          |
|--|--|
| 1. **全面性**：覆盖所有参数组合，确保找到全局最优解（在定义的参数空间内）。 | 1. **计算成本高**：参数维度增加时，组合数指数级增长（维度灾难）。 |
| 2. **易于实现**：逻辑简单，适合超参数数量较少的情况。 | 2. **效率低**：可能大量时间浪费在无效参数上（如学习率过高导致模型不收敛）。 |
| 3. **可并行化**：不同参数组合相互独立，适合分布式计算。 | 3. **依赖参数空间定义**：若预设范围不合理，可能错过最优解。 |



## **4. 网格搜索 vs 其他方法**
| **方法**         | **适用场景**                              | **效率**          | **搜索策略**              |
||--|-|--|
| **网格搜索**     | 参数空间小（如2-4个参数）                | 低效（穷举）      | 遍历所有组合             |
| **随机搜索**     | 参数空间大（超参数重要性差异大）         | 高效              | 随机抽样组合             |
| **贝叶斯优化**   | 参数空间大且评估成本高（如深度学习调参） | 最高效（自适应）  | 基于概率模型引导搜索方向 |



## **5. 实践建议**
1. **先粗调后精调**：  
   - 先在大范围、少候选值下粗调，缩小最优参数范围后再精细搜索。  
   - 例如：首轮搜索学习率 `[0.001, 0.01, 0.1]`，确定最优在 `0.01` 附近后，细化到 `[0.005, 0.01, 0.02]`。

2. **结合领域知识**：  
   - 根据模型特性限制参数范围（如SVM的 `C` 通常在 `[1e-3, 1e3]` 之间）。

3. **并行加速**：  
   - 利用 `n_jobs=-1`（Scikit-learn）或分布式计算框架（如Dask）加速搜索。

4. **替代方案选择**：  
   - 参数超过4个时，优先使用随机搜索或贝叶斯优化。



## **6. 示例场景**
假设用支持向量机（SVM）分类：
```python
from sklearn.svm import SVC

param_grid = {
    'C': [0.1, 1, 10],              # 正则化强度
    'kernel': ['linear', 'rbf'],     # 核函数类型
    'gamma': [0.01, 0.1, 'scale']    # 核函数系数
}

grid_search = GridSearchCV(SVC(), param_grid, cv=5)
grid_search.fit(X_train, y_train)
```



## **总结**
网格搜索是超参数调优的“暴力”但可靠的方法，**适合参数少、计算资源充足**的场景。在实际应用中，需权衡计算成本与模型性能，必要时结合随机搜索或贝叶斯优化提高效率。