# Bagging

装袋算法（Bagging，全称Bootstrap Aggregating）是一种集成学习方法，旨在通过组合多个基学习器的预测结果来提高模型的稳定性和泛化能力。其核心思想是通过对训练数据的有放回采样生成多个子数据集，分别训练基学习器，最终通过投票（分类）或平均（回归）进行结果聚合。


## 1. 核心思想
Bagging的核心是降低模型方差（Variance），尤其适用于高方差、低偏差的模型（如深度决策树）。通过以下两步实现：
1. Bootstrap采样：从原始训练集中有放回地随机抽取样本，生成多个子训练集（每个子集大小与原始训练集相同）。
2. 聚合（Aggregating）：基于各子集训练多个基学习器，通过投票或平均合并预测结果。


## 2. 算法流程
1. 生成Bootstrap样本：
   - 从原始数据集 $D$ 中有放回地抽取 $n$ 个样本，生成 $m$ 个子训练集 $D_1, D_2, ..., D_m$。
   - 每个子集的样本数通常与原始数据集相同（约63.2%的原始样本会被选中，其余为袋外样本OOB）。

2. 训练基学习器：
   - 对每个子集 $D_i$，独立训练一个基学习器 $h_i$（如决策树、神经网络等）。
   - 基学习器需为不稳定模型（即对数据扰动敏感，如决策树），否则Bagging效果有限。

3. 结果聚合：
   - 分类任务：多数投票法（Majority Voting）。
   - 回归任务：简单平均法（Averaging）。


## 3. 数学原理
- 方差-偏差分解：Bagging通过降低方差提升泛化性能。假设基学习器独立，其方差为 $\sigma^2$，则集成后的方差为 $\sigma^2 / m$（理想情况下）。
- Bootstrap采样：样本重复概率约为 $1 - (1 - 1/n)^n \approx 1 - 1/e \approx 63.2\%$，未被选中的样本（OOB）可用于验证。


## 4. 优势与适用场景
- 优势：
  - 降低方差：尤其适用于高方差模型（如未剪枝的决策树）。
  - 抗过拟合：通过平均噪声影响减少过拟合风险。
  - 并行化：基学习器独立训练，适合分布式计算。
  - 灵活性：支持任意基学习器。

- 适用场景：
  - 数据噪声较大时。
  - 基模型复杂度高（如深度决策树）。
  - 需要提升模型稳定性（如金融风控、医疗诊断）。



## 5. 局限性
- 对偏差无效：若基学习器本身偏差高（如线性回归），Bagging无法显著改善。
- 计算成本高：需训练多个模型，资源消耗较大。
- 解释性差：集成结果难以直观解释。


## 6. 典型变种：随机森林（Random Forest）
- 改进点：
  - 在Bagging基础上，引入特征随机选择（每棵树分裂时仅用部分特征）。
  - 进一步降低基学习器的相关性，增强多样性。
- 效果：
  - 比普通Bagging方差更低，泛化能力更强。


## 7. 与Boosting对比
| 特性       | Bagging               | Boosting               |
|----------------|---------------------------|----------------------------|
| 训练方式   | 并行                      | 串行（逐步修正错误）       |
| 目标       | 降低方差                  | 降低偏差                   |
| 基学习器   | 独立且同质                | 依赖前序结果，加权训练     |
| 数据权重   | 均匀采样                  | 错误样本权重增加           |
| 代表算法   | 随机森林                  | AdaBoost、GBDT、XGBoost    |


## 8. 代码示例（Python）
```python {cmd="python3"}
from sklearn.ensemble import BaggingClassifier
from sklearn.tree import DecisionTreeClassifier

# 基学习器：决策树
base_model = DecisionTreeClassifier()
# Bagging集成
bagging = BaggingClassifier(
    base_model, 
    n_estimators=100, 
    max_samples=0.8, 
    bootstrap=True
)
bagging.fit(X_train, y_train)
```


## 总结
Bagging通过Bootstrap采样和结果聚合，显著降低模型方差，尤其适用于复杂模型和高噪声数据。其变种随机森林进一步结合特征随机性，成为实际应用中的高效工具。然而，在偏差主导的场景中需结合Boosting等其他方法。